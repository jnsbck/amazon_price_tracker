{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.request import HTTPError # for catching timeout for website response\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError # error produced if empty csv if parsed\n",
    "import time # for sleep function\n",
    "from datetime import datetime # for timestamp\n",
    "import matplotlib.pyplot as plt # for plotting price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tracker:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.items = {\"nicknames\": [], \"names\": [], \"urls\": []}\n",
    "        self.price_history = {}\n",
    "        self.__retrieve_items()\n",
    "        \n",
    "        DateTime = [\"year\", \"month\", \"day\", \"hour\", \"minute\"]\n",
    "        self.price_history = pd.DataFrame(columns=DateTime+self.items[\"nicknames\"])\n",
    "        \n",
    "        self.__retrieve_price_hist()\n",
    "\n",
    "    def add_item(self, URL, nickname):\n",
    "        if URL not in self.items[\"urls\"] and nickname not in self.items[\"nicknames\"]:\n",
    "            try:\n",
    "                with urllib.request.urlopen(URL) as f:\n",
    "                    html_doc = f.read()\n",
    "\n",
    "                soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "                # extract name\n",
    "                for element in soup.find_all(\"span\"):\n",
    "                    if \"productTitle\" in str(element):\n",
    "                        title_containing_str = str(element)\n",
    "                        break\n",
    "                title_containing_str_start = title_containing_str.find(\">\")+1\n",
    "                title_containing_str_end = title_containing_str.find(\"</\")\n",
    "                title = title_containing_str[title_containing_str_start:title_containing_str_end].replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
    "                \n",
    "                # save title and URL to txt\n",
    "                f = open(\"tracked_items.txt\",\"a\", newline=\"\\n\")\n",
    "                if title not in self.items[\"names\"]:\n",
    "                    f.write(nickname + \" : \" + title + \" : \" + URL + \"\\n\")\n",
    "                f.close()\n",
    "\n",
    "                # save title and URL to dict\n",
    "                if title not in self.items[\"names\"]:\n",
    "                    self.items[\"names\"].append(title)\n",
    "                    self.items[\"urls\"].append(URL)\n",
    "                    self.items[\"nicknames\"].append(nickname)\n",
    "\n",
    "            except HTTPError:\n",
    "                print(\"HTTP 503 Error, try to add item again later.\")\n",
    "        else:\n",
    "            print(\"This item is already being tracked.\")\n",
    "            \n",
    "\n",
    "            \n",
    "    def __retrieve_items(self):\n",
    "        # retrieve tracked items\n",
    "        try:\n",
    "            f = open(\"tracked_items.txt\", \"r\")\n",
    "            if f.read() == \"\":\n",
    "                print(\"No items are being tracked so far. Please add an item to be tracked using .add_item().\")\n",
    "                f.close()\n",
    "            else:\n",
    "                f = open(\"tracked_items.txt\", \"r\")\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    nickname, title, url = line.split(\" : \")\n",
    "                    if title not in self.items[\"names\"]:\n",
    "                        self.items[\"names\"].append(title)\n",
    "                        self.items[\"urls\"].append(url)\n",
    "                        self.items[\"nicknames\"].append(nickname)\n",
    "            f.close()\n",
    "        except FileNotFoundError:\n",
    "            open(\"tracked_items.txt\", \"x\")\n",
    "    \n",
    "    def __retrieve_price_hist(self):\n",
    "        try:\n",
    "            self.price_history = pd.read_csv(\"price_history.csv\")\n",
    "        except FileNotFoundError:\n",
    "            open(\"price_history.csv\", \"x\")\n",
    "        except EmptyDataError:\n",
    "            if len(self.items[\"names\"]) > 0:\n",
    "                print(\"The price history is empty so far. Please fetch prices using .fetch_prices() first.\")\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "        \n",
    "    def wipe_database(self):\n",
    "        # delete contents of files\n",
    "        items = open(\"tracked_items.txt\", \"w\")\n",
    "        items.write(\"\")\n",
    "        items.close()\n",
    "        \n",
    "        hist = open(\"price_history.csv\", \"w\")\n",
    "        hist.write(\"\")\n",
    "        hist.close()\n",
    "        \n",
    "    def fetch_prices(self):  \n",
    "        # extract price\n",
    "        tries = 0\n",
    "        max_tries = 10\n",
    "        delay = 1 # delay between fetching items in s\n",
    "        DateTime = [\"year\", \"month\", \"day\", \"hour\", \"minute\"]\n",
    "        new_row = pd.DataFrame(columns=DateTime+self.items[\"nicknames\"])\n",
    "        while tries < max_tries:\n",
    "            try:\n",
    "                if len(self.items[\"names\"]) > 0:\n",
    "                    for n in range(len(self.items[\"urls\"])):\n",
    "                        URL = self.items[\"urls\"][n]\n",
    "                        try:\n",
    "                            with urllib.request.urlopen(URL) as f:\n",
    "                                html_doc = f.read()\n",
    "\n",
    "                            soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "                            time.sleep(delay)\n",
    "\n",
    "                            for element in soup.find_all(\"span\"):\n",
    "                                if \"a-color-price price\" in str(element):\n",
    "                                    price_containing_str = str(element)\n",
    "                                    break\n",
    "\n",
    "                            price_str = ''.join([i for i in list(price_containing_str) if i.isdigit()])\n",
    "                            price_str = price_str[:-2] + \".\" + price_str[-2:]\n",
    "                            price = float(price_str)\n",
    "                            item_name = self.items[\"nicknames\"][n]\n",
    "                            new_row[item_name] = [price]\n",
    "                        except HTTPError:\n",
    "                            item_name = self.items[\"nicknames\"][n]\n",
    "                            new_row[item_name] = [np.NaN]\n",
    "                else:\n",
    "                    print(\"There is no items to fetch a price for. Please add items using .add_item() first.\")\n",
    "                    break\n",
    "                    \n",
    "                now = datetime.now()\n",
    "                datetime_vec = now.timetuple()[0:5]\n",
    "                new_row[DateTime] = datetime_vec\n",
    "                new_row.index = range(self.price_history.shape[0],self.price_history.shape[0]+1)\n",
    "                self.price_history = self.price_history.append(new_row, sort=False)\n",
    "                \n",
    "                # save price history\n",
    "                self.price_history.to_csv(\"price_history.csv\", index_label=False)\n",
    "                break\n",
    "\n",
    "            except HTTPError:\n",
    "                print(\"HTTP 503 Error, trying again in 5 minutes.\")\n",
    "                time.sleep(5*60)\n",
    "                tries += 1\n",
    "                \n",
    "    def remove_item(self):\n",
    "        print(\"The items currently being tracked are: \\n\")\n",
    "        for i in range(len(self.items[\"nicknames\"])):\n",
    "            print(\"[\" + str(i) + \"] --> \" + self.items[\"nicknames\"][i])\n",
    "        Input = input(\"\\nTo remove an item from tracking enter the corresponding number.\\nTo cancel, press 'Enter' \")\n",
    "        if Input.isdigit():\n",
    "            item2delete_idx = int(Input)\n",
    "            if item2delete_idx < len(self.items[\"nicknames\"]):\n",
    "                item_name = self.items[\"nicknames\"][item2delete_idx]\n",
    "\n",
    "                # remove from hist\n",
    "                self.price_history = self.price_history.drop(item_name, axis=1)\n",
    "                \n",
    "                # remove from tracked items\n",
    "                self.items[\"names\"].pop(item2delete_idx)\n",
    "                self.items[\"nicknames\"].pop(item2delete_idx)\n",
    "                self.items[\"urls\"].pop(item2delete_idx)\n",
    "                \n",
    "                # remove from corresponding .txt and .csv\n",
    "                f_read = open(\"tracked_items.txt\", \"r\")\n",
    "                lines = f_read.readlines()\n",
    "                lines.pop(item2delete_idx)\n",
    "                f_write = open(\"tracked_items.txt\", \"w\")\n",
    "                f_write.write(\"\".join(lines))\n",
    "                f_read.close()\n",
    "                f_write.close()\n",
    "                \n",
    "                self.price_history.to_csv(\"price_history.csv\", index_label=False)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                print(\"Item was removed.\")\n",
    "            else:\n",
    "                print(\"The input does not correspond to an item.\")\n",
    "        elif Input == \"\":\n",
    "            print(\"The action has been canceled.\")\n",
    "        else:\n",
    "            print(\"The input is not valid.\")\n",
    "        \n",
    "\n",
    "    def plot_prices(self, timescale=\"day\"):\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        time_axis = self.price_history[timescale]\n",
    "        tracked_items = list(tracker.price_history.columns)[5:]\n",
    "        for item in tracked_items:\n",
    "            plt.plot(time_axis,tracker.price_history[item], \"-o\" , label=item)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.xlabel(timescale + \"s\")\n",
    "        plt.ylabel(\"Price in â‚¬\")\n",
    "        plt.show()\n",
    "\n",
    "#     def deploy(self):\n",
    "#         loop  once every day, if not looping ask for items to add\n",
    "        \n",
    "    \n",
    "# add functionality to see how many items are left in stock if possible!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price history is empty so far. Please fetch prices using .fetch_prices() first.\n"
     ]
    }
   ],
   "source": [
    "tracker = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL1 = \"https://www.amazon.de/dp/B07H289S79/ref=sr_1_1_sspa?crid=10NNT7QDBOWO4&keywords=4+TB+Seagate&qid=1576506626&sprefix=seagate+%2Caps%2C182&sr=8-1-spons&psc=1&spLa=ZW5jcnlwdGVkUXVhbGlmaWVyPUEzMFZLNUFTSk5VQ0xFJmVuY3J5cHRlZElkPUEwNTEyOTI2M01OWldDVDI4SkYxMCZlbmNyeXB0ZWRBZElkPUEwNDkxOTQ0MVM0UVZGR0hKWkZUUSZ3aWRnZXROYW1lPXNwX2F0ZiZhY3Rpb249Y2xpY2tSZWRpcmVjdCZkb05vdExvZ0NsaWNrPXRydWU=\"\n",
    "URL2 = \"https://www.amazon.de/gp/product/B07SXMZLPK/ref=ox_sc_saved_title_3?smid=A133RA3ZUAU4I7&psc=1\"\n",
    "URL3 = \"https://www.amazon.de/gp/product/B07MFBLN7K/ref=ox_sc_saved_title_4?smid=A1NW99WOC7UQ45&psc=1\"\n",
    "tracker.add_item(URL1, \"Seagate 4TB HDD\")\n",
    "tracker.add_item(URL2, \"Ryzen 3700x\")\n",
    "tracker.add_item(URL3, \"500 GB Samsung SSD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year month day hour minute  Seagate 4TB HDD  500 GB Samsung SSD\n",
      "0  2019    12  16   23     53           109.99              117.63\n"
     ]
    }
   ],
   "source": [
    "# tracker.fetch_prices()\n",
    "print(tracker.price_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
