{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # for plotting price data\n",
    "import pandas as pd\n",
    "from pandas.errors import EmptyDataError # error produced if empty csv if parsed\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests # fetches html content of a website, instead of urllib2 previously\n",
    "from urllib.request import HTTPError # for catching timeout for website response\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import URLError\n",
    "\n",
    "import time # for sleep function\n",
    "from datetime import datetime # for timestamp\n",
    "\n",
    "import os # for creation of directories\n",
    "import re # for regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker\n",
    "- price_hist\n",
    "- tracked items\n",
    "    - deploy\n",
    "    - current_prices\n",
    "    - add and remove items\n",
    "    - fetch prices\n",
    "    - reset\n",
    "    - retrieve price hist and items from file\n",
    "    - log activity\n",
    "#### Components (sub-classes)\n",
    "- notify\n",
    "    - daily/weekly etc. (incl. plot?)\n",
    "    - notify if price hike or drop for a certain item\n",
    "    - notify if tracker goes down somehow\n",
    "- connectivity\n",
    "    - check connectivity\n",
    "- item\n",
    "    - price\n",
    "    - url\n",
    "    - ASIN\n",
    "    - nickname\n",
    "    - name\n",
    "- scraper\n",
    "    - find price\n",
    "    - find items left\n",
    "    - find prices of other vendors?\n",
    "- visualise\n",
    "    - different options to plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonPriceTracker:\n",
    "    \n",
    "    def __init__(self, tracker_name=\"tracker\"):\n",
    "        self.items = {\"nicknames\": [], \"names\": [], \"asins\": [], \"urls\": []}\n",
    "        self.name = tracker_name\n",
    "        self.PATH = \"./\" + self.name + \"/\"\n",
    "        try:\n",
    "            os.mkdir(str(self.name))\n",
    "        except FileExistsError:\n",
    "            print(\"This tracker already exists. Using the existing one instead.\")\n",
    "        \n",
    "        self.price_history = {}\n",
    "        self.__retrieve_items()\n",
    "        \n",
    "        DateTime = [\"year\", \"month\", \"day\", \"hour\", \"minute\"]\n",
    "        self.price_history = pd.DataFrame(columns=DateTime+self.items[\"nicknames\"])\n",
    "        \n",
    "        self.__retrieve_price_hist()\n",
    "        self.latest_prices = self.price_history.tail(1)\n",
    "        \n",
    "    def __webpage2html(self, URL, parser=\"html.parser\"):\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "        }\n",
    "        res = requests.get(URL, headers=headers)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(res.text, parser)\n",
    "        return soup\n",
    "\n",
    "     \n",
    "    def add_item(self, URL, nickname):\n",
    "        if \"amazon\" not in URL:\n",
    "            print(\"This is not a valid amazon url.\")\n",
    "        else:\n",
    "            ASIN = URL.split(\"/\")[4]\n",
    "            URL = \"/\".join(URL.split(\"/\")[:5])\n",
    "            if ASIN not in self.items[\"asins\"]:\n",
    "                print(\"Adding item to list of tracked items.\")\n",
    "                try:\n",
    "                    soup = self.__webpage2html(URL)\n",
    "\n",
    "                    # extract name\n",
    "                    for element in soup.find_all(\"span\"):\n",
    "                        if \"productTitle\" in str(element):\n",
    "                            title_containing_str = str(element)\n",
    "                            break\n",
    "                    title_containing_str_start = title_containing_str.find(\">\")+1\n",
    "                    title_containing_str_end = title_containing_str.find(\"</\")\n",
    "                    title_raw = title_containing_str[title_containing_str_start:title_containing_str_end]\n",
    "                    title = title_raw.replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
    "\n",
    "                    # save title and URL to txt\n",
    "                    f = open(self.PATH + \"tracked_items.txt\",\"a\", newline=\"\\n\")\n",
    "                    if title not in self.items[\"names\"]:\n",
    "                        f.write(nickname + \" : \" + title + \" : \" + URL + \" : \" + ASIN + \"\\n\")\n",
    "                    f.close()\n",
    "\n",
    "                    # save title and URL to dict\n",
    "                    self.items[\"names\"].append(title)\n",
    "                    self.items[\"urls\"].append(URL)\n",
    "                    self.items[\"nicknames\"].append(nickname)\n",
    "                    self.items[\"asins\"].append(ASIN)\n",
    "                    print(\"{} was succesfully added to list of tracked items.\".format(nickname))\n",
    "\n",
    "                except HTTPError:\n",
    "                    print(\"HTTP 503 Error, try to add item again later.\")\n",
    "            else:\n",
    "                print(\"This item is already being tracked.\")\n",
    "            \n",
    "            \n",
    "    def __retrieve_items(self):\n",
    "        # retrieve tracked items\n",
    "        try:\n",
    "            f = open(self.PATH + \"tracked_items.txt\", \"r\")\n",
    "            if f.read() == \"\":\n",
    "                print(\"No items are being tracked so far. \\\n",
    "                Please add an item to be tracked using .add_item().\")\n",
    "                f.close()\n",
    "            else:\n",
    "                f = open(self.PATH + \"tracked_items.txt\", \"r\")\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    nickname, title, url, asin = line.split(\" : \")\n",
    "                    if asin[:-1] not in self.items[\"asins\"]:\n",
    "                        self.items[\"names\"].append(title)\n",
    "                        self.items[\"urls\"].append(url)\n",
    "                        self.items[\"nicknames\"].append(nickname)\n",
    "                        self.items[\"asins\"].append(asin[:-1])\n",
    "            f.close()\n",
    "        except FileNotFoundError:\n",
    "            open(self.PATH + \"tracked_items.txt\", \"x\")\n",
    "    \n",
    "    \n",
    "    def __retrieve_price_hist(self):\n",
    "        try:\n",
    "            self.price_history = pd.read_csv(self.PATH + \"price_history.csv\")\n",
    "        except FileNotFoundError:\n",
    "            open(self.PATH + \"price_history.csv\", \"x\")\n",
    "        except EmptyDataError:\n",
    "            if len(self.items[\"names\"]) > 0:\n",
    "                print(\"The price history is empty so far. \\\n",
    "                Please fetch prices using .fetch_prices() first.\")\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "    def wipe_database(self):\n",
    "        # delete contents of files\n",
    "        items = open(self.PATH + \"tracked_items.txt\", \"w\")\n",
    "        items.write(\"\")\n",
    "        items.close()\n",
    "        \n",
    "        hist = open(self.PATH + \"price_history.csv\", \"w\")\n",
    "        hist.write(\"\")\n",
    "        hist.close()\n",
    "        \n",
    "        \n",
    "    def fetch_prices(self, URLs=None):  \n",
    "        # extract price\n",
    "        if URLs is None:\n",
    "            URLs = self.items[\"urls\"]\n",
    "        error_status = None\n",
    "        delay = 1 # delay between fetching items in s\n",
    "        DateTime = [\"year\", \"month\", \"day\", \"hour\", \"minute\"]\n",
    "        new_row = pd.DataFrame(columns=DateTime+self.items[\"nicknames\"])\n",
    "        if len(self.items[\"names\"]) > 0:\n",
    "            for n, URL in enumerate(URLs):\n",
    "                try:\n",
    "                    print(\"Fetching price for {}.\".format(self.items[\"nicknames\"][n]))\n",
    "                    soup = self.__webpage2html(URL, \"html.parser\")\n",
    "                    time.sleep(delay)\n",
    "                    item_name = self.items[\"nicknames\"][n]\n",
    "                    try:\n",
    "                        price_str = soup.select(\"#priceblock_ourprice\")[0].text.replace(\",\",\".\")\n",
    "                        price = float(price_str[:price_str.index(\".\")+3])\n",
    "                        new_row[item_name] = [price]\n",
    "                    except IndexError:\n",
    "                        print(\"The item or price is currently unavailable.\")\n",
    "                        new_row[item_name] = [np.NaN]\n",
    "                    \n",
    "                except HTTPError:\n",
    "                    item_name = self.items[\"nicknames\"][n]\n",
    "                    new_row[item_name] = [np.NaN]\n",
    "                    print(\"\\n A price for {} could not be fetched.\".format(item_name))\n",
    "                    error_status = True\n",
    "        \n",
    "            now = datetime.now()\n",
    "            datetime_vec = now.timetuple()[0:5]\n",
    "            new_row[DateTime] = datetime_vec\n",
    "            new_row.index = range(self.price_history.shape[0],self.price_history.shape[0]+1)\n",
    "            self.price_history = self.price_history.append(new_row, sort=False, ignore_index=True)\n",
    "            self.latest_prices = self.price_history.tail(1)\n",
    "\n",
    "            # save price history\n",
    "            self.price_history.to_csv(self.PATH + \"price_history.csv\", index_label=False, index=False)\n",
    "    \n",
    "        else:\n",
    "            print(\"There is no items to fetch a price for. Please add items using .add_item() first.\")\n",
    "    \n",
    "        return error_status\n",
    "                \n",
    "        \n",
    "    def remove_item(self):\n",
    "        print(\"The items currently being tracked are: \\n\")\n",
    "        for i in range(len(self.items[\"nicknames\"])):\n",
    "            print(\"[\" + str(i) + \"] --> \" + self.items[\"nicknames\"][i])\n",
    "        Input = input(\"\\n To remove an item from tracking enter the corresponding number.\\\n",
    "        \\n To cancel, press 'Enter'. \")\n",
    "        if Input.isdigit():\n",
    "            item2delete_idx = int(Input)\n",
    "            if item2delete_idx < len(self.items[\"nicknames\"]):\n",
    "                item_name = self.items[\"nicknames\"][item2delete_idx]\n",
    "\n",
    "                # remove from hist\n",
    "                self.price_history = self.price_history.drop(item_name, axis=1)\n",
    "                \n",
    "                # remove from tracked items\n",
    "                self.items[\"names\"].pop(item2delete_idx)\n",
    "                self.items[\"nicknames\"].pop(item2delete_idx)\n",
    "                self.items[\"urls\"].pop(item2delete_idx)\n",
    "                self.items[\"asins\"].pop(item2delete_idx)\n",
    "                \n",
    "                # remove from corresponding .txt and .csv\n",
    "                f_read = open(self.PATH + \"tracked_items.txt\", \"r\")\n",
    "                lines = f_read.readlines()\n",
    "                lines.pop(item2delete_idx)\n",
    "                f_write = open(self.PATH + \"tracked_items.txt\", \"w\")\n",
    "                f_write.write(\"\".join(lines))\n",
    "                f_read.close()\n",
    "                f_write.close()\n",
    "                \n",
    "                self.price_history.to_csv(self.PATH + \"price_history.csv\", index_label=False)\n",
    "                \n",
    "                print(\"Item was removed.\")\n",
    "            else:\n",
    "                print(\"The input does not correspond to an item.\")\n",
    "        elif Input == \"\":\n",
    "            print(\"The action has been canceled.\")\n",
    "        else:\n",
    "            print(\"The input is not valid.\")\n",
    "        \n",
    "\n",
    "    def plot_prices(self, timescale=\"day\"):\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        time_axis = self.price_history[timescale]\n",
    "        tracked_items = list(self.price_history.columns)[5:]\n",
    "        for item in tracked_items:\n",
    "            plt.plot(time_axis,self.price_history[item], \"-o\" , label=item)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.xlabel(timescale + \"s\")\n",
    "        plt.ylabel(\"Price in €\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def current_prices(self):\n",
    "        self.fetch_prices()\n",
    "        current_price = self.latest_prices\n",
    "        return current_price\n",
    "    \n",
    "    def __internet_on(self):\n",
    "        try:\n",
    "            urlopen('http://216.58.192.142', timeout=1)\n",
    "            return True\n",
    "        except URLError as err: \n",
    "            return False\n",
    "\n",
    "        \n",
    "    def deploy(self):\n",
    "        while True:\n",
    "            _time = datetime.now().timetuple()[2:5]\n",
    "            today = _time[0]\n",
    "            hour = _time[1]\n",
    "            minute = _time[2]\n",
    "            try:\n",
    "                prev_year, prev_month, prev_day, *_ = np.loadtxt(self.PATH + \"price_history.csv\", skiprows=1, delimiter=\",\")[-1]\n",
    "            except TypeError:\n",
    "                prev_year, prev_month, prev_day, *_ = np.loadtxt(self.PATH + \"price_history.csv\", skiprows=1, delimiter=\",\")\n",
    "            except StopIteration:\n",
    "                prev_year, prev_month, prev_day = -1, -1, -1\n",
    "            print(\"Checking time...\")\n",
    "            if hour == 0 and (minute < 59 and minute > 0):\n",
    "                if prev_day != today:\n",
    "                    attempt = 1\n",
    "                    URLs = np.array(self.items[\"urls\"])\n",
    "                    while attempt < 10:\n",
    "                        try:\n",
    "                            print(\"Attempt {} to fetch prices.\".format(attempt))\n",
    "                            status = self.fetch_prices(URLs)\n",
    "                            if status == None:\n",
    "                                print(\"Fetching was a success!\")\n",
    "                                print(\"...waiting for next fetch.\")\n",
    "                                break\n",
    "                            else:\n",
    "                                latest_prices = self.price_history.iloc[-1,5:]\n",
    "                                fails = np.array(latest_prices.isna())\n",
    "                                URLs = URLs[fails]\n",
    "                                attempt += 1\n",
    "                                nicknames_of_fails = np.array(self.items[\"nicknames\"])[fails]\n",
    "                                print(\"Encountered an error while fetching prices for {}. Trying again in 10 min.\".format(list(nicknames_of_fails)))\n",
    "                                time.sleep(10*60)\n",
    "                        except HTTPError:\n",
    "                            print(\"HTTP 503 Error, trying again in 10 minutes.\")\n",
    "                            attempt += 1\n",
    "                            time.sleep(10*60)\n",
    "                else:\n",
    "                    print(\"Item prices have already been updated today.\")\n",
    "            time.sleep(59*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def notify(self, email):\n",
    "#             send email with price plot\n",
    "\n",
    "#     def request_update(self.):\n",
    "#         update prices and send email with current prices and history of them at the request\n",
    "    \n",
    "# add functionality to see how many items are left in stock if possible!!!\n",
    "# add functionality to recieve an email every day with plot of price developement and if anything as changed\n",
    "# add functionality to compare prices to other vendors\n",
    "# e.g. open with urllib https://www.amazon.de/gp/offer-listing/ (ASIN --> B07SXMZLPK) /ref=dp_olp_new_mbc?ie=UTF8&condition=new\n",
    "# and scrape webpage for all the prices in soup.select(\"#olpOfferList\")[0].div.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item():\n",
    "    def __init__(self, nickname=None, description=None, url=None, asin=None, price=None, currency=None, last_updated=None,created=None):\n",
    "        self.Nickname = nickname\n",
    "        self.Description = description\n",
    "        self.Asin = asin\n",
    "        self.Url = url\n",
    "        self.Price = price\n",
    "        self.Currency = currency\n",
    "        self.Last_updated = last_updated\n",
    "        self.Created = created\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str({\n",
    "                \"Nickname\": self.Nickname,\n",
    "                \"Description\": self.Description,\n",
    "                \"Asin\": self.Asin,\n",
    "                \"Url\": self.Url,\n",
    "                \"Price\": self.Price, \n",
    "                \"Currency\": self.Currency,\n",
    "                \"Created\": self.Created,\n",
    "                \"Last_updated\": self.Last_updated\n",
    "               })\n",
    "\n",
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        self.Online = False\n",
    "        \n",
    "    def webpage2soup(self, url, parser=\"html.parser\"):\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "        }\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(res.text, parser)\n",
    "        return soup\n",
    "            \n",
    "    def test_connection(self, url='http://216.58.192.142'):\n",
    "        try:\n",
    "            urlopen(url, timeout=1)\n",
    "            self.Online = True\n",
    "        except URLError as err:\n",
    "            self.Online = False\n",
    "        return self.Online\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.Template_url = r\"(https://)*(www.)*([a-z_-]+)\\.([a-z]+)/([a-z0-9-_]+)/([a-z0-9-_]+)/([a-z0-9-_]+)\" # Amazon regex\n",
    "        self.Template_description = r\"(<span class=\\\"a-size-large\\\" id=\\\"productTitle\\\">)(\\n\\s+)([A-Za-z0-9, ]+)(\\n\\s+)(</span>)\"\n",
    "        \n",
    "    def find_attributes(self, html):\n",
    "        attributes = {\n",
    "                \"description\": \"\",\n",
    "                \"currency\": \"\", \n",
    "               }\n",
    "        \n",
    "           \n",
    "        return attributes\n",
    "    \n",
    "    def parse_url(self,url):\n",
    "        regex = re.compile(self.Pattern)\n",
    "        m = regex.search(url.lower())\n",
    "        url_slices = m.groups()\n",
    "        \n",
    "        topleveldomain = \".\" + url_slices[3]\n",
    "        \n",
    "        if url_slices[3] == \"de\":\n",
    "            if url_slices[4] == \"gp\":\n",
    "                asin = url_slices[6]\n",
    "            else:\n",
    "                asin = url_slices[5]\n",
    "        elif url_slices[3] == \"com\":\n",
    "            asin = url_slices[6]\n",
    "        else:\n",
    "            pass # so far only .com and .de supported\n",
    "            \n",
    "        return asin, topleveldomain\n",
    "    \n",
    "    def find_currency(self, html):\n",
    "        pass\n",
    "    \n",
    "    def find_price(self, html):\n",
    "        pass\n",
    "    \n",
    "    def find_description(self, html):\n",
    "        regex = re.compile(self.Template_description)\n",
    "        m = regex.search(str(html))\n",
    "        return m.groups()[2] # holds the product description\n",
    "        \n",
    "class Notifier():\n",
    "    def __init__(self):\n",
    "        pass  \n",
    "\n",
    "    \n",
    "class Tracker(Item,Scraper,Notifier,Parser):\n",
    "    def __init__(self, path=\".\", name=\"default_tracker\"):\n",
    "        self.Path = path\n",
    "        self.Name = name\n",
    "        self.Items = []\n",
    "        \n",
    "        Scraper.__init__(self)\n",
    "        Parser.__init__(self)\n",
    "        \n",
    "    def add_item(self, nickname=None, description=None, url=None, asin=None, price=None, currency=None, last_updated=None, created=None):\n",
    "        item = Item(nickname, description, url, asin, price, currency, last_updated, created)\n",
    "        self.Items.append(item)\n",
    "    \n",
    "    def add_item_by_url(self, alias, url):\n",
    "        html = self.webpage2soup(url)\n",
    "        attributes = self.find_attributes(html)\n",
    "        \n",
    "        nickname = alias\n",
    "        description = attributes[\"description\"]\n",
    "        currency = attributes[\"currency\"]\n",
    "        asin, _ = self.parse_url(url)\n",
    "        created = datetime.now().timetuple()[0:5]\n",
    "        \n",
    "        self.add_item(nickname, description, url, asin, None, currency, created, created)\n",
    "        \n",
    "    def deploy(self):\n",
    "        pass\n",
    "    \n",
    "    def fetch_prices(self):\n",
    "        pass\n",
    "    \n",
    "    def load(self, path):\n",
    "        pass\n",
    "    \n",
    "    def save(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 43), match='https://www.amazon.de/gp/product/b07sxmzlpk'>\n",
      "<re.Match object; span=(0, 39), match='https://www.amazon.de/dp/b07stggq18/ref'>\n",
      "<re.Match object; span=(0, 71), match='https://www.amazon.com/amd-ryzen-3600-12-thread-p>\n",
      "('https://', 'www.', 'amazon', 'de', 'gp', 'product', 'b07sxmzlpk')\n",
      "('https://', 'www.', 'amazon', 'de', 'dp', 'b07stggq18', 'ref')\n",
      "('https://', 'www.', 'amazon', 'com', 'amd-ryzen-3600-12-thread-processor', 'dp', 'b07stggq18')\n"
     ]
    }
   ],
   "source": [
    "testurl = \"https://www.amazon.de/gp/product/B07SXMZLPK/ref=ox_sc_saved_title_1?smid=A27FVGL1U6882E&psc=1\"\n",
    "testurl2 = \"https://www.amazon.de/dp/B07STGGQ18/ref=sr_1_3?crid=3FQI4QYW0HJRH&dchild=1&keywords=ryzen+5+3600&qid=1584471580&sprefix=ry%2Caps%2C201&sr=8-3\"\n",
    "testurl3 = \"https://www.amazon.com/AMD-Ryzen-3600-12-Thread-Processor/dp/B07STGGQ18/ref=sr_1_2?crid=2D1P6H7XKJW3A&keywords=ryzen+5+3600&qid=1584472320&sprefix=ryz%2Caps%2C452&sr=8-2\"\n",
    "regex = re.compile(r\"(https://)*(www.)*([a-z_-]+)\\.([a-z]+)/([a-z0-9-_]+)/([a-z0-9-_]+)/([a-z0-9-_]+)\")\n",
    "m = regex.search(testurl.lower())\n",
    "m2 = regex.search(testurl2.lower())\n",
    "m3 = regex.search(testurl3.lower())\n",
    "print(m)\n",
    "print(m2)\n",
    "print(m3)\n",
    "print(m.groups())\n",
    "print(m2.groups())\n",
    "print(m3.groups())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.add_item_by_url(\"test\", \"https://www.amazon.de/gp/product/B07SXMZLPK/ref=ox_sc_saved_title_1?smid=A27FVGL1U6882E&psc=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraper.webpage2soup(\"https://www.amazon.de/gp/product/B07SXMZLPK/ref=ox_sc_saved_title_1?smid=A27FVGL1U6882E&psc=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = Scraper()\n",
    "html = scraper.webpage2soup(testurl)\n",
    "html2 = scraper.webpage2soup(testurl2)\n",
    "html3 = scraper.webpage2soup(testurl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"a-size-medium a-color-price priceBlockBuyingPriceString\" id=\"priceblock_ourprice\">319,00 €</span>]"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = r\"(<span class=([\\\"a-zA-z0-9-]+)*(</span>)\"\n",
    "\n",
    "price_str = html.select(\"#priceblock_ourprice\")\n",
    "price_str        \n",
    "# regex = re.compile(pattern)\n",
    "# m = regex.search(html)\n",
    "# print(m.groups())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<span class=\"a-size-large\" id=\"productTitle\">\\n                \\n                    \\n                    \\n                \\n\\n                \\n                    \\n                    \\n                        AMD Ryzen 7 3700X Prozessor, 4GHz AM4 36MB Cache Wraith Prism\\n                    \\n                \\n\\n                \\n                    \\n                    \\n                \\n            </span>'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_containing_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
