{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt # for plotting price data\n",
    "# from pandas.errors import EmptyDataError # error produced if empty csv if parsed\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests # fetches html content of a website, instead of urllib2 previously\n",
    "from urllib.request import HTTPError # for catching timeout for website response\n",
    "from urllib.request import urlopen\n",
    "from urllib.request import URLError\n",
    "\n",
    "import time # for sleep function\n",
    "from datetime import datetime # for timestamp\n",
    "\n",
    "import os # for creation of directories\n",
    "import re # for regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker\n",
    "- price_hist\n",
    "- tracked items\n",
    "    - deploy\n",
    "    - current_prices\n",
    "    - add and remove items\n",
    "        - if item already exists, ask if it should be overwritten\n",
    "    - fetch prices\n",
    "    - reset\n",
    "    - retrieve price hist and items from file\n",
    "    - log activity\n",
    "#### Components (sub-classes)\n",
    "- notify\n",
    "    - daily/weekly etc. (incl. plot?)\n",
    "    - notify if price hike or drop for a certain item\n",
    "    - notify if tracker goes down somehow\n",
    "- connectivity\n",
    "    - check connectivity\n",
    "- item\n",
    "    - price\n",
    "    - url\n",
    "    - ASIN\n",
    "    - nickname\n",
    "    - name\n",
    "- scraper\n",
    "    - find price\n",
    "    - find items left\n",
    "    - find prices of other vendors?\n",
    "- visualise\n",
    "    - different options to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Dos:\n",
    "- Prevent addidtion of duplicates\n",
    "- add ouput of messages and add them to log file\n",
    "- maybe standardise product URLs after input. That means amazon. tplvldomain / sth / ASIN for every product\n",
    "- notification functionality via email\n",
    "- remove eval methods and change how the items are stored\n",
    "- catch error between individual fetches or between updates and act accordingly, e.g. only fetch missing item again\n",
    "- add documentation and better commenting\n",
    "- add more prompts/log opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonPriceTracker:\n",
    "    \n",
    "    def __init__(self, tracker_name=\"tracker\"):\n",
    "        self.items = {\"nicknames\": [], \"names\": [], \"asins\": [], \"urls\": []}\n",
    "        self.name = tracker_name\n",
    "        self.PATH = \"./\" + self.name + \"/\"\n",
    "        try:\n",
    "            os.mkdir(str(self.name))\n",
    "        except FileExistsError:\n",
    "            print(\"This tracker already exists. Using the existing one instead.\")\n",
    "        \n",
    "        self.price_history = {}\n",
    "        self.__retrieve_items()\n",
    "        \n",
    "        DateTime = [\"year\", \"month\", \"day\", \"hour\", \"minute\"]\n",
    "        self.price_history = pd.DataFrame(columns=DateTime+self.items[\"nicknames\"])\n",
    "        \n",
    "        self.__retrieve_price_hist()\n",
    "        self.latest_prices = self.price_history.tail(1)\n",
    "        \n",
    "    def __webpage2html(self, URL, parser=\"html.parser\"):\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "        }\n",
    "        res = requests.get(URL, headers=headers)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(res.text, parser)\n",
    "        return soup\n",
    "\n",
    "     \n",
    "    def add_item(self, URL, nickname):\n",
    "        if \"amazon\" not in URL:\n",
    "            print(\"This is not a valid amazon url.\")\n",
    "        else:\n",
    "            ASIN = URL.split(\"/\")[4]\n",
    "            URL = \"/\".join(URL.split(\"/\")[:5])\n",
    "            if ASIN not in self.items[\"asins\"]:\n",
    "                print(\"Adding item to list of tracked items.\")\n",
    "                try:\n",
    "                    soup = self.__webpage2html(URL)\n",
    "\n",
    "                    # extract name\n",
    "                    for element in soup.find_all(\"span\"):\n",
    "                        if \"productTitle\" in str(element):\n",
    "                            title_containing_str = str(element)\n",
    "                            break\n",
    "                    title_containing_str_start = title_containing_str.find(\">\")+1\n",
    "                    title_containing_str_end = title_containing_str.find(\"</\")\n",
    "                    title_raw = title_containing_str[title_containing_str_start:title_containing_str_end]\n",
    "                    title = title_raw.replace(\"\\n\", \"\").replace(\"  \", \"\")\n",
    "\n",
    "                    # save title and URL to txt\n",
    "                    f = open(self.PATH + \"tracked_items.txt\",\"a\", newline=\"\\n\")\n",
    "                    if title not in self.items[\"names\"]:\n",
    "                        f.write(nickname + \" : \" + title + \" : \" + URL + \" : \" + ASIN + \"\\n\")\n",
    "                    f.close()\n",
    "\n",
    "                    # save title and URL to dict\n",
    "                    self.items[\"names\"].append(title)\n",
    "                    self.items[\"urls\"].append(URL)\n",
    "                    self.items[\"nicknames\"].append(nickname)\n",
    "                    self.items[\"asins\"].append(ASIN)\n",
    "                    print(\"{} was succesfully added to list of tracked items.\".format(nickname))\n",
    "\n",
    "                except HTTPError:\n",
    "                    print(\"HTTP 503 Error, try to add item again later.\")\n",
    "            else:\n",
    "                print(\"This item is already being tracked.\")\n",
    "            \n",
    "            \n",
    "    def __retrieve_items(self):\n",
    "        # retrieve tracked items\n",
    "        try:\n",
    "            f = open(self.PATH + \"tracked_items.txt\", \"r\")\n",
    "            if f.read() == \"\":\n",
    "                print(\"No items are being tracked so far. \\\n",
    "                Please add an item to be tracked using .add_item().\")\n",
    "                f.close()\n",
    "            else:\n",
    "                f = open(self.PATH + \"tracked_items.txt\", \"r\")\n",
    "                lines = f.readlines()\n",
    "                for line in lines:\n",
    "                    nickname, title, url, asin = line.split(\" : \")\n",
    "                    if asin[:-1] not in self.items[\"asins\"]:\n",
    "                        self.items[\"names\"].append(title)\n",
    "                        self.items[\"urls\"].append(url)\n",
    "                        self.items[\"nicknames\"].append(nickname)\n",
    "                        self.items[\"asins\"].append(asin[:-1])\n",
    "            f.close()\n",
    "        except FileNotFoundError:\n",
    "            open(self.PATH + \"tracked_items.txt\", \"x\")\n",
    "    \n",
    "    \n",
    "    def __retrieve_price_hist(self):\n",
    "        try:\n",
    "            self.price_history = pd.read_csv(self.PATH + \"price_history.csv\")\n",
    "        except FileNotFoundError:\n",
    "            open(self.PATH + \"price_history.csv\", \"x\")\n",
    "        except EmptyDataError:\n",
    "            if len(self.items[\"names\"]) > 0:\n",
    "                print(\"The price history is empty so far. \\\n",
    "                Please fetch prices using .fetch_prices() first.\")\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "    def wipe_database(self):\n",
    "        # delete contents of files\n",
    "        items = open(self.PATH + \"tracked_items.txt\", \"w\")\n",
    "        items.write(\"\")\n",
    "        items.close()\n",
    "        \n",
    "        hist = open(self.PATH + \"price_history.csv\", \"w\")\n",
    "        hist.write(\"\")\n",
    "        hist.close()\n",
    "        \n",
    "        \n",
    "    def fetch_prices(self, URLs=None):  \n",
    "        # extract price\n",
    "        if URLs is None:\n",
    "            URLs = self.items[\"urls\"]\n",
    "        error_status = None\n",
    "        delay = 1 # delay between fetching items in s\n",
    "        DateTime = [\"year\", \"month\", \"day\", \"hour\", \"minute\"]\n",
    "        new_row = pd.DataFrame(columns=DateTime+self.items[\"nicknames\"])\n",
    "        if len(self.items[\"names\"]) > 0:\n",
    "            for n, URL in enumerate(URLs):\n",
    "                try:\n",
    "                    print(\"Fetching price for {}.\".format(self.items[\"nicknames\"][n]))\n",
    "                    soup = self.__webpage2html(URL, \"html.parser\")\n",
    "                    time.sleep(delay)\n",
    "                    item_name = self.items[\"nicknames\"][n]\n",
    "                    try:\n",
    "                        price_str = soup.select(\"#priceblock_ourprice\")[0].text.replace(\",\",\".\")\n",
    "                        price = float(price_str[:price_str.index(\".\")+3])\n",
    "                        new_row[item_name] = [price]\n",
    "                    except IndexError:\n",
    "                        print(\"The item or price is currently unavailable.\")\n",
    "                        new_row[item_name] = [np.NaN]\n",
    "                    \n",
    "                except HTTPError:\n",
    "                    item_name = self.items[\"nicknames\"][n]\n",
    "                    new_row[item_name] = [np.NaN]\n",
    "                    print(\"\\n A price for {} could not be fetched.\".format(item_name))\n",
    "                    error_status = True\n",
    "        \n",
    "            now = datetime.now()\n",
    "            datetime_vec = now.timetuple()[0:5]\n",
    "            new_row[DateTime] = datetime_vec\n",
    "            new_row.index = range(self.price_history.shape[0],self.price_history.shape[0]+1)\n",
    "            self.price_history = self.price_history.append(new_row, sort=False, ignore_index=True)\n",
    "            self.latest_prices = self.price_history.tail(1)\n",
    "\n",
    "            # save price history\n",
    "            self.price_history.to_csv(self.PATH + \"price_history.csv\", index_label=False, index=False)\n",
    "    \n",
    "        else:\n",
    "            print(\"There is no items to fetch a price for. Please add items using .add_item() first.\")\n",
    "    \n",
    "        return error_status\n",
    "                \n",
    "        \n",
    "    def remove_item(self):\n",
    "        print(\"The items currently being tracked are: \\n\")\n",
    "        for i in range(len(self.items[\"nicknames\"])):\n",
    "            print(\"[\" + str(i) + \"] --> \" + self.items[\"nicknames\"][i])\n",
    "        Input = input(\"\\n To remove an item from tracking enter the corresponding number.\\\n",
    "        \\n To cancel, press 'Enter'. \")\n",
    "        if Input.isdigit():\n",
    "            item2delete_idx = int(Input)\n",
    "            if item2delete_idx < len(self.items[\"nicknames\"]):\n",
    "                item_name = self.items[\"nicknames\"][item2delete_idx]\n",
    "\n",
    "                # remove from hist\n",
    "                self.price_history = self.price_history.drop(item_name, axis=1)\n",
    "                \n",
    "                # remove from tracked items\n",
    "                self.items[\"names\"].pop(item2delete_idx)\n",
    "                self.items[\"nicknames\"].pop(item2delete_idx)\n",
    "                self.items[\"urls\"].pop(item2delete_idx)\n",
    "                self.items[\"asins\"].pop(item2delete_idx)\n",
    "                \n",
    "                # remove from corresponding .txt and .csv\n",
    "                f_read = open(self.PATH + \"tracked_items.txt\", \"r\")\n",
    "                lines = f_read.readlines()\n",
    "                lines.pop(item2delete_idx)\n",
    "                f_write = open(self.PATH + \"tracked_items.txt\", \"w\")\n",
    "                f_write.write(\"\".join(lines))\n",
    "                f_read.close()\n",
    "                f_write.close()\n",
    "                \n",
    "                self.price_history.to_csv(self.PATH + \"price_history.csv\", index_label=False)\n",
    "                \n",
    "                print(\"Item was removed.\")\n",
    "            else:\n",
    "                print(\"The input does not correspond to an item.\")\n",
    "        elif Input == \"\":\n",
    "            print(\"The action has been canceled.\")\n",
    "        else:\n",
    "            print(\"The input is not valid.\")\n",
    "        \n",
    "\n",
    "    def plot_prices(self, timescale=\"day\"):\n",
    "        fig = plt.figure(figsize=(10,6))\n",
    "        time_axis = self.price_history[timescale]\n",
    "        tracked_items = list(self.price_history.columns)[5:]\n",
    "        for item in tracked_items:\n",
    "            plt.plot(time_axis,self.price_history[item], \"-o\" , label=item)\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.xlabel(timescale + \"s\")\n",
    "        plt.ylabel(\"Price in €\")\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def current_prices(self):\n",
    "        self.fetch_prices()\n",
    "        current_price = self.latest_prices\n",
    "        return current_price\n",
    "    \n",
    "    def __internet_on(self):\n",
    "        try:\n",
    "            urlopen('http://216.58.192.142', timeout=1)\n",
    "            return True\n",
    "        except URLError as err: \n",
    "            return False\n",
    "\n",
    "        \n",
    "    def deploy(self):\n",
    "        while True:\n",
    "            _time = datetime.now().timetuple()[2:5]\n",
    "            today = _time[0]\n",
    "            hour = _time[1]\n",
    "            minute = _time[2]\n",
    "            try:\n",
    "                prev_year, prev_month, prev_day, *_ = np.loadtxt(self.PATH + \"price_history.csv\", skiprows=1, delimiter=\",\")[-1]\n",
    "            except TypeError:\n",
    "                prev_year, prev_month, prev_day, *_ = np.loadtxt(self.PATH + \"price_history.csv\", skiprows=1, delimiter=\",\")\n",
    "            except StopIteration:\n",
    "                prev_year, prev_month, prev_day = -1, -1, -1\n",
    "            print(\"Checking time...\")\n",
    "            if hour == 0 and (minute < 59 and minute > 0):\n",
    "                if prev_day != today:\n",
    "                    attempt = 1\n",
    "                    URLs = np.array(self.items[\"urls\"])\n",
    "                    while attempt < 10:\n",
    "                        try:\n",
    "                            print(\"Attempt {} to fetch prices.\".format(attempt))\n",
    "                            status = self.fetch_prices(URLs)\n",
    "                            if status == None:\n",
    "                                print(\"Fetching was a success!\")\n",
    "                                print(\"...waiting for next fetch.\")\n",
    "                                break\n",
    "                            else:\n",
    "                                latest_prices = self.price_history.iloc[-1,5:]\n",
    "                                fails = np.array(latest_prices.isna())\n",
    "                                URLs = URLs[fails]\n",
    "                                attempt += 1\n",
    "                                nicknames_of_fails = np.array(self.items[\"nicknames\"])[fails]\n",
    "                                print(\"Encountered an error while fetching prices for {}. Trying again in 10 min.\".format(list(nicknames_of_fails)))\n",
    "                                time.sleep(10*60)\n",
    "                        except HTTPError:\n",
    "                            print(\"HTTP 503 Error, trying again in 10 minutes.\")\n",
    "                            attempt += 1\n",
    "                            time.sleep(10*60)\n",
    "                else:\n",
    "                    print(\"Item prices have already been updated today.\")\n",
    "            time.sleep(59*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def notify(self, email):\n",
    "#             send email with price plot\n",
    "\n",
    "#     def request_update(self.):\n",
    "#         update prices and send email with current prices and history of them at the request\n",
    "    \n",
    "# add functionality to see how many items are left in stock if possible!!!\n",
    "# add functionality to recieve an email every day with plot of price developement and if anything as changed\n",
    "# add functionality to compare prices to other vendors\n",
    "# e.g. open with urllib https://www.amazon.de/gp/offer-listing/ (ASIN --> B07SXMZLPK) /ref=dp_olp_new_mbc?ie=UTF8&condition=new\n",
    "# and scrape webpage for all the prices in soup.select(\"#olpOfferList\")[0].div.div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Item():\n",
    "    def __init__(self, nickname=None, description=None, url=None, asin=None, price=None, currency=None, last_updated=None,created=None):\n",
    "        self.Nickname = nickname\n",
    "        self.Description = description\n",
    "        self.Asin = asin\n",
    "        self.Url = url\n",
    "        self.Price = price\n",
    "        self.Currency = currency\n",
    "        self.Created = created\n",
    "        self.Last_updated = last_updated\n",
    "        self.Price_log = {\"timestamp\": [last_updated], \"price\": [price]}\n",
    "        self.DatetimeFormatStr = \"%H:%M, %m/%d/%Y\"# \"(%Y, %m, %d, %H, %M)\" # temporary better: \"%H:%M, %m/%d/%Y\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str({\n",
    "                \"Nickname\": self.Nickname,\n",
    "                \"Description\": self.Description,\n",
    "                \"Asin\": self.Asin,\n",
    "                \"Url\": self.Url,\n",
    "                \"Price\": self.Price, \n",
    "                \"Currency\": self.Currency,\n",
    "                \"Created\": self.Created.strftime(self.DatetimeFormatStr),\n",
    "                \"Last_updated\": self.Last_updated.strftime(self.DatetimeFormatStr)\n",
    "               })\n",
    "    \n",
    "    def from_txt(self, file):\n",
    "        with open(file, \"r\") as f:\n",
    "            class_attrs = eval(f.readline()) # eval is always dangerous! temporary\n",
    "            self.Price_log = eval(f.readline()) # eval is always dangerous! temporary\n",
    "            for index, (timestamp,price) in enumerate(zip(self.Price_log[\"timestamp\"], self.Price_log[\"price\"])):\n",
    "                self.Price_log[\"timestamp\"][index] = datetime.strptime(timestamp, self.DatetimeFormatStr) # str neccesary because of eval()\n",
    "                self.Price_log[\"price\"][index] = float(price)\n",
    "        \n",
    "        self.Nickname =  class_attrs[\"Nickname\"]\n",
    "        self.Description = class_attrs[\"Description\"]\n",
    "        self.Asin = class_attrs[\"Asin\"]\n",
    "        self.Url = class_attrs[\"Url\"]\n",
    "        self.Price = float(class_attrs[\"Price\"])\n",
    "        self.Currency = class_attrs[\"Currency\"]\n",
    "        self.Created = datetime.strptime(str(class_attrs[\"Created\"]), self.DatetimeFormatStr) # str neccesary because of eval()\n",
    "        self.Last_updated = datetime.strptime(str(class_attrs[\"Last_updated\"]), self.DatetimeFormatStr) # str neccesary because of eval()\n",
    "\n",
    "    def __reformat_date(self, date):\n",
    "        return datetime.strftime(date, self.DatetimeFormatStr)\n",
    "\n",
    "    def to_txt(self, path=\"./\"):\n",
    "        with open(path + self.Nickname + \".txt\", \"w\") as f:\n",
    "            f.write(self.__str__() + \"\\n\")\n",
    "            price_log = self.Price_log\n",
    "            price_log[\"timestamp\"] = list(map(self.__reformat_date, price_log[\"timestamp\"]))\n",
    "            f.write(str(price_log)) # temporary solution\n",
    "\n",
    "class Scraper():\n",
    "    def __init__(self):\n",
    "        self.Online = False\n",
    "        \n",
    "    def webpage2soup(self, url, parser=\"lxml\"):\n",
    "        headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36',\n",
    "        }\n",
    "        res = requests.get(url, headers=headers)\n",
    "        res.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(res.text, parser)\n",
    "        return soup\n",
    "            \n",
    "    def test_connection(self, url='http://216.58.192.142'):\n",
    "        try:\n",
    "            urlopen(url, timeout=1)\n",
    "            self.Online = True\n",
    "        except URLError as err:\n",
    "            self.Online = False\n",
    "        return self.Online\n",
    "    \n",
    "    def ping_AmazonDE(self):\n",
    "        return test_connection(\"amazon.de\")\n",
    "\n",
    "class Parser():\n",
    "    def __init__(self):\n",
    "        self.Template_Amazon_Url = r\"(https://)*(www.)*([a-z_-]+)\\.([a-z]+)/([a-z0-9-_]+)/([a-z0-9-_]+)/([a-z0-9-_]+)\" # Amazon regex\n",
    "        self.Template_Amazon_Description = r\"(<span\\s(class=\\\"a-size-large\\\"\\s)*(id=\\\"productTitle\\\")(\\sclass=\\\"a-size-large\\\")*>\\n\\s+(.+)\\n\\s+</span>)\"\n",
    "        self.Template_Amazon_Price = r\"([0-9,]+)\\s(.)\"\n",
    "        \n",
    "    def __groupbytemplate(self, string, re_template):\n",
    "        regex = re.compile(re_template)\n",
    "        m = regex.search(string)\n",
    "        return m.groups()        \n",
    "        \n",
    "    def find_attributes(self, html):\n",
    "        attributes = {\"description\": \"\",\n",
    "                      \"currency\": \"\",\n",
    "                      \"price\": \"\"}\n",
    "        \n",
    "        # find product description\n",
    "        description = self.find_description(html)\n",
    "        attributes[\"description\"] = description\n",
    "        \n",
    "        # find price and currency\n",
    "        price, currency = self.find_price(html)\n",
    "        attributes[\"price\"] = float(price)\n",
    "        attributes[\"currency\"] = currency\n",
    "           \n",
    "        return attributes\n",
    "    \n",
    "    def parse_url(self,url):\n",
    "        regex = re.compile(self.Template_Amazon_Url)\n",
    "        m = regex.search(url.lower())\n",
    "        url_slices = m.groups()\n",
    "        \n",
    "        topleveldomain = \".\" + url_slices[3]\n",
    "        \n",
    "        if url_slices[3] == \"de\":\n",
    "            if url_slices[4] == \"gp\":\n",
    "                asin = url_slices[6]\n",
    "            else:\n",
    "                asin = url_slices[5]\n",
    "        elif url_slices[3] == \"com\":\n",
    "            asin = url_slices[6]\n",
    "        else:\n",
    "            pass # so far only .com and .de supported\n",
    "            \n",
    "        return asin, topleveldomain\n",
    "    \n",
    "    def find_price(self, html):\n",
    "        price_str = str(html.select(\"span#priceblock_ourprice\"))\n",
    "        groups = self.__groupbytemplate(price_str, self.Template_Amazon_Price)\n",
    "        price = groups[0].replace(\",\", \".\")\n",
    "        currency = groups[1]\n",
    "        return price, currency\n",
    "    \n",
    "    def find_description(self, html):\n",
    "        title_str = \"\" # why do I have to reference this var before?\n",
    "        for element in html.find_all(\"span\"):\n",
    "            if \"productTitle\" in str(element):\n",
    "                title_str = str(element)\n",
    "                break\n",
    "        groups = self.__groupbytemplate(title_str, self.Template_Amazon_Description)\n",
    "        description = groups[4]\n",
    "        return description\n",
    "        \n",
    "class Notifier():\n",
    "    def __init__(self, path=\"./\", logfile=\"events\"):\n",
    "        self.Last_event = (None, None) # event + timestamp\n",
    "        open(path + logfile + \".log\", \"a\")\n",
    "        self.Log_path = path\n",
    "        self.Logfile_name = logfile\n",
    "        \n",
    "    def prompt(self, event=\"\"):\n",
    "        timestamp = datetime.now()\n",
    "        print(timestamp.strftime(\"%H:%M, %m/%d/%Y\") + \" -- \" + event)\n",
    "        self.Last_event = (timestamp, event)\n",
    "        return timestamp, event\n",
    "    \n",
    "    def log(self, event=\"\"):\n",
    "        timestamp, event = self.prompt(event)\n",
    "        with open(self.Log_path + self.Logfile_name + \".log\", \"a\") as f:\n",
    "            f.write(str(timestamp) + \" -- \" + event + \"\\n\")\n",
    "        pass\n",
    "    \n",
    "    def send_email(self):\n",
    "        pass\n",
    "    \n",
    "class Tracker(Item, Scraper, Notifier, Parser):\n",
    "    def __init__(self, path=\"./\", name=\"default_tracker\", load=False):\n",
    "        self.Path = path + name + \"/\"\n",
    "        self.Name = name\n",
    "        self.Items = []\n",
    "        \n",
    "        if load:\n",
    "            self.load(self.Path)\n",
    "        else:\n",
    "            try:\n",
    "                os.mkdir(self.Path)\n",
    "            except FileExistsError:\n",
    "                print(\"This tracker already exists.\") # do you want to load it?\n",
    "                \n",
    "        Scraper.__init__(self)\n",
    "        Parser.__init__(self)\n",
    "        Notifier.__init__(self, self.Path)\n",
    "        \n",
    "    def add_item(self, nickname=None, description=None, url=None, asin=None, price=None, currency=None, last_updated=None, created=None, save=False):\n",
    "        item = Item(nickname, description, url, asin, price, currency, last_updated, created)\n",
    "        self.Items.append(item)\n",
    "        \n",
    "        if save:\n",
    "            item.to_txt(self.Path)\n",
    "    \n",
    "    def __asin(self, item):\n",
    "        return item.Asin\n",
    "    \n",
    "    def add_item_by_url(self, alias, url, save=False):\n",
    "        asin, _ = self.parse_url(url)\n",
    "        if asin not in list(map(self.__asin,tracker.Items)):\n",
    "            html = self.webpage2soup(url)\n",
    "            attributes = self.find_attributes(html)\n",
    "\n",
    "            nickname = alias\n",
    "            description = attributes[\"description\"]\n",
    "            price = attributes[\"price\"]\n",
    "            currency = attributes[\"currency\"]\n",
    "            created = datetime.now()\n",
    "\n",
    "            self.add_item(nickname, description, url, asin, price, currency, created, created, save)\n",
    "            self.log(nickname + \" was successfully added to: \" + self.Name + \".\")\n",
    "        else:\n",
    "            self.log(\"ASIN matches an item that is already being tracḱed.\")\n",
    "        \n",
    "    def list_items(self):\n",
    "        for item in self.Items:\n",
    "            print(item.Nickname)\n",
    "    \n",
    "    def fetch_price(self, Item):\n",
    "        html = self.webpage2soup(Item.Url)\n",
    "        price, currency = self.find_price(html)\n",
    "        self.log(\"The Price for \" + Item.Nickname + \" has been successfully fetched.\")\n",
    "        return price, currency\n",
    "    \n",
    "    def update_prices(self, timeb4nextfetch=0):\n",
    "        now = datetime.now()\n",
    "        for Item in self.Items:\n",
    "            try:\n",
    "                price, _ = self.fetch_price(Item)\n",
    "                Item.Price = price\n",
    "            except:\n",
    "                Item.Price = np.nan\n",
    "                self.log(\"The Price for \" + Item.Nickname + \" could not be fetched.\")\n",
    "                \n",
    "            Item.Last_updated = now\n",
    "            Item.Price_log[\"timestamp\"].append(now)\n",
    "            Item.Price_log[\"price\"].append(price)\n",
    "            time.sleep(timeb4nextfetch)\n",
    "                \n",
    "    def deploy(self):\n",
    "        self.log(self.Name + \" has been deployed.\")\n",
    "        while(True):\n",
    "            if self.test_connection(url=\"amazon.de\"):\n",
    "                self.update_prices(5)\n",
    "                self.log(\"Prices have been updated.\")\n",
    "                self.save()\n",
    "                self.log(\"New Prices have been saved.\")\n",
    "                self.history_to_csv(True)\n",
    "                time.sleep(60*60*12)\n",
    "                self.log(\"Waiting 12 hours for next update...\")\n",
    "            else:\n",
    "                time.sleep(60*10)\n",
    "                self.log(\"Could not establish connection with Amazon, waiting 10min before trying again...\")\n",
    "    \n",
    "    def load(self, path):\n",
    "        self.Path = path\n",
    "        regex = re.compile(r\"/([a-zA-Z0-9-_]+)/$\")\n",
    "        m = regex.search(path)\n",
    "        self.Name = m.groups()[0]\n",
    "        if len(self.Items) == 0:\n",
    "            files_in_dir = [f for f in os.listdir(self.Path) if os.path.isfile(os.path.join(self.Path, f))]\n",
    "            for file in files_in_dir:\n",
    "                if file[-4:] == \".txt\":\n",
    "                    item = Item()\n",
    "                    item.from_txt(self.Path + file)\n",
    "                    self.Items.append(item)\n",
    "        self.log(self.Name + \" has been successfully loaded.\")\n",
    "                    \n",
    "    def save(self):\n",
    "        for item in self.Items:\n",
    "            item.to_txt(self.Path)\n",
    "    \n",
    "    def history_to_csv(self, save=False):  \n",
    "        df = pd.DataFrame({})\n",
    "        for item in self.Items:\n",
    "            dct = {item.Nickname: []}\n",
    "            timestamps, prices = item.Price_log.values()\n",
    "            for timestamp, price in zip(timestamps, prices):\n",
    "                dct[item.Nickname].append(price)\n",
    "            df_col = pd.DataFrame(dct, index=[timestamp.strftime(Item().DatetimeFormatStr)])\n",
    "            df = pd.concat([df, df_col])\n",
    "        # merge rows with the same timestamp\n",
    "        for timestamp in df.index.unique():\n",
    "            n_prices = df.loc[timestamp].notna().sum() # implement check, to prevent 0s !!!\n",
    "            df = (df.loc[timestamp].fillna(0).sum() / n_prices).to_frame(timestamp).T\n",
    "        df.index.name = \"timestamp\"\n",
    "        if save:\n",
    "            df.to_csv(self.Path + \"price_hist.csv\")\n",
    "            self.log(\"Price history has been saved to .csv\")\n",
    "            \n",
    "        return df         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This tracker already exists.\n"
     ]
    }
   ],
   "source": [
    "tracker = Tracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracker.add_item_by_url(\"AMD Ryzen 7 3700x\", \"https://www.amazon.de/gp/product/B07SXMZLPK/ref=ox_sc_saved_title_1?smid=A27FVGL1U6882E&psc=1\")\n",
    "# tracker.add_item_by_url(\"32GB DDR4 RAM\", \"https://www.amazon.de/gp/product/B016ORTNI2/ref=ox_sc_saved_title_4?smid=A3JWKAKR8XB7XF&psc=1\")\n",
    "# tracker.add_item_by_url(\"512 GB M.2 SSD\", \"https://www.amazon.de/gp/product/B07CJ3RVP3/ref=ox_sc_saved_title_5?smid=A3JWKAKR8XB7XF&psc=1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01:27, 03/30/2020 -- default_tracker has been successfully loaded.\n",
      "01:27, 03/30/2020 -- The Price for 512 GB M.2 SSD has been successfully fetched.\n",
      "01:27, 03/30/2020 -- The Price for 32GB DDR4 RAM has been successfully fetched.\n",
      "01:27, 03/30/2020 -- The Price for AMD Ryzen 7 3700x has been successfully fetched.\n",
      "512 GB M.2 SSD\n",
      "32GB DDR4 RAM\n",
      "AMD Ryzen 7 3700x\n"
     ]
    }
   ],
   "source": [
    "# tracker.save()\n",
    "tracker.load(\"./default_tracker/\")\n",
    "tracker.update_prices()\n",
    "tracker.list_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
